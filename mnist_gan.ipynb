{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_side_dimension = 28\n",
    "h_dim_gen = 256\n",
    "h_dim_disc = 256\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xavier_init = tf.contrib.layers.xavier_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discriminator netowrk\n",
    "X = tf.placeholder(tf.float32, shape=[None, 28 * 28], name='X')\n",
    "D_W1 = tf.Variable(xavier_init([28*28, h_dim_disc]), name='D_W1')\n",
    "D_b1 = tf.Variable(tf.zeros(shape=[h_dim_disc]), name='D_b1')\n",
    "D_W2 = tf.Variable(xavier_init([h_dim_disc, 1]), name='D_W2')\n",
    "D_b2 = tf.Variable(tf.zeros(shape=[1]), name='D_b2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_theta = [D_W1, D_b1, D_W2, D_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator network\n",
    "Z = tf.placeholder(tf.float32, shape=[None, 100], name='Z')\n",
    "G_W1 = tf.Variable(xavier_init([100, h_dim_gen]), name='G_W1')\n",
    "G_b1 = tf.Variable(tf.zeros(shape=[h_dim_gen]), name='G_b1')\n",
    "G_W2 = tf.Variable(xavier_init([h_dim_gen, 28 * 28]), name='G_W2')\n",
    "G_b2 = tf.Variable(tf.zeros(shape=[28 * 28]), name='G_b2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_theta = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    D_h1 = tf.nn.relu(tf.matmul(x, D_W1) + D_b1)\n",
    "    D_logit = tf.matmul(D_h1, D_W2) + D_b2\n",
    "    D_prob = tf.sigmoid(D_logit)\n",
    "    \n",
    "    return D_prob, D_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):\n",
    "    G_h1 = tf.nn.relu(tf.matmul(z, G_W1) + G_b1)\n",
    "    G_logit = tf.matmul(G_h1, G_W2) + G_b2\n",
    "    G_prob = tf.sigmoid(G_logit)\n",
    "    \n",
    "    return G_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_Z(m, n):\n",
    "    return np.random.uniform(-1.0, 1.0, size=[m, n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_sample = generator(Z)\n",
    "D_real, D_logit_real = discriminator(X)\n",
    "D_fake, D_logit_fake = discriminator(G_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_loss = -tf.reduce_mean(tf.log(D_real) + tf.log(1.0 - D_fake))\n",
    "G_loss = -tf.reduce_mean(tf.log(D_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(D_loss, var_list=D_theta)\n",
    "G_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(G_loss, var_list=G_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iters = 10000\n",
    "minibatch_size = 128\n",
    "Z_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('../out/'):\n",
    "    os.makedirs('../out/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "from matplotlib.pyplot import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for it in range(1000000):\n",
    "#     if it % 100 == 0:\n",
    "#         samples = sess.run(G_sample, feed_dict={Z: sample_Z(16, Z_dim)})\n",
    "\n",
    "#         fig = plot(samples)\n",
    "#         plt.savefig('../out/{}.png'.format(str(i).zfill(3)), bbox_inches='tight')\n",
    "#         i += 1\n",
    "#         plt.show()\n",
    "#         plt.close(fig)\n",
    "\n",
    "#     X_mb, _ = mnist.train.next_batch(minibatch_size)\n",
    "\n",
    "#     _, D_loss_curr = sess.run([D_optimizer, D_loss], feed_dict={X: X_mb, Z: sample_Z(minibatch_size, Z_dim)})\n",
    "#     _, G_loss_curr = sess.run([G_optimizer, G_loss], feed_dict={Z: sample_Z(minibatch_size, Z_dim)})\n",
    "\n",
    "#     if it % 100 == 0:\n",
    "#         print('Iter: {}'.format(it))\n",
    "#         print('D loss: {:.4}'. format(D_loss_curr))\n",
    "#         print('G_loss: {:.4}'.format(G_loss_curr))\n",
    "#         print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch G loss at step 0: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/matplotlib/image.py:405: UserWarning: Warning: converting a masked element to nan.\n",
      "  dv = (np.float64(self.norm.vmax) -\n",
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/matplotlib/image.py:406: UserWarning: Warning: converting a masked element to nan.\n",
      "  np.float64(self.norm.vmin))\n",
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/matplotlib/image.py:412: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_min = np.float64(newmin)\n",
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/matplotlib/image.py:417: UserWarning: Warning: converting a masked element to nan.\n",
      "  a_max = np.float64(newmax)\n",
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/matplotlib/colors.py:916: UserWarning: Warning: converting a masked element to nan.\n",
      "  dtype = np.min_scalar_type(value)\n",
      "/home/mahlet/anaconda3/lib/python3.6/site-packages/numpy/ma/core.py:718: UserWarning: Warning: converting a masked element to nan.\n",
      "  data = np.array(a, copy=False, subok=subok)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch G loss at step 200: nan\n",
      "Minibatch G loss at step 400: nan\n",
      "Minibatch G loss at step 600: nan\n",
      "Minibatch G loss at step 800: nan\n",
      "Minibatch G loss at step 1000: nan\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x216 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch G loss at step 1200: nan\n",
      "Minibatch G loss at step 1400: nan\n",
      "Minibatch G loss at step 1600: nan\n"
     ]
    }
   ],
   "source": [
    "for iter in range(num_iters):\n",
    "    X_batch, _ = mnist.train.next_batch(minibatch_size)\n",
    "    Z_sample = sample_Z(minibatch_size, 100)\n",
    "    _, D_loss_curr = session.run([D_optimizer, D_loss], feed_dict={X: X_batch, Z: Z_sample})\n",
    "    _, G_loss_curr = session.run([G_optimizer, G_loss], feed_dict={Z: Z_sample})\n",
    "    \n",
    "    if (iter%200) == 0:\n",
    "#         print(\"Minibatch D loss at step %d: %f\" % (iter, D_loss_curr))\n",
    "        print(\"Minibatch G loss at step %d: %f\" % (iter, G_loss_curr))\n",
    "\n",
    "    if (iter%1000) == 0:\n",
    "        n = 3\n",
    "        canvas = np.empty((28 * n, 28 * n))\n",
    "        for i in range(n):\n",
    "            Z_test_sample = sample_Z(n, 100)\n",
    "            G_test_sample = session.run(G_sample, feed_dict={Z: Z_test_sample})\n",
    "            \n",
    "            G_test_sample = np.multiply((G_test_sample - 1.0), 1.0)\n",
    "            for j in range(n):\n",
    "                canvas[i*image_side_dimension:(i+1)*image_side_dimension, j*image_side_dimension:(j+1)*image_side_dimension] = G_test_sample[j].reshape([image_side_dimension, image_side_dimension])\n",
    "        \n",
    "        plt.figure(figsize=(n, n))\n",
    "        plt.imshow(canvas, origin=\"upper\", cmap=\"gray\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
